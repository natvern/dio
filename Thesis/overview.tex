\section{Symbolic Reasoning for Reinforcement Learning} 
\label{symrl}

To tackle the challenges from Section~\ref{sec:challenges}, we are
inspired by the current Neurosymbolic AI trends, which explore
combinations of deep learning (DL) and symbolic reasoning.
%
The work has been a response to criticism of DL on its lack of formal
semantics and intuitive explanation, and the lack of expert knowledge
towards guiding machine learning models.
%
A key question the field targets is identifying the necessary and
sufficient building blocks of AI~\cite{garcez2020neurosymbolic},
namely, how can we provide the semantics of knowledge, and work
towards meta-learning? 
%
Current Neurosymbolic AI trends are concerned with knowledge representation and reasoning, namely, they investigate computational-logic systems 
and representation to precede learning in order to provide some form
of incremental update, e.g. a meta-network to group two sub-neural
networks~\cite{Besold2017NeuralSymbolicLA}.
As a result, neurosymbolic AI has been successfully applied to vision-based tasks such as semantic labeling \cite{vinyals2015, karpathy2015}, 
vision analogy-making \cite{Reed2015DeepVA}, or learning communication
protocols \cite{Foerster2016LearningTC}.
Those results inspire us to use those techniques for reinforcement learning, as to tackle its challenges.

\medskip
Specifcally, we tackle the challenge of finding a systematic method to map a state to a numerical value: a reward. 
To do so, we make use of the fact that rewards are domain dependent and thus, given domain specific rules, a \emph{domain informed} module can guide a RL agent towards better decisions. This can be done by 
adapting the reward function. For instance, we consider defining which states are desirable, which are to be avoided and which are fatal. Given rules and judgments, a logic programming module 
is able to search the space and send feedback to the reinforcement learning agent. The goal is a systematic method to design a reward function which can ensure faster and more efficient 
training. This knowledge can furthermore be incorporated into resolving the exploration vs. exploitation dilemma. For instance, if a domain informed module 
can infer that only one of the possible next states is desirable, then exploration in that specific case is suboptimal.  
We will call the proposed module a \emph{Domain Informed Oracle}
(\dio{}).

\begin{figure}[H]
  \centering
  \includegraphics[scale=0.45]{figures/overview.png}
  \caption{Overview of the proposed solution}
  \label{fig:overview}
\end{figure}


The diagram in Figure~\ref{fig:overview} is a high-level description of our proposed solution. 
%
Precisely, the (state, action) pair is fed into \dio{}. While the environment produces its own 
reward function, \dio{} also computes its own reward. 

Those two rewards are merged by the reward shaping unit. We could either 
overwrite the initial reward and only trust \dio{}, likewise we could trust them both equally and take their average. This is left as a \emph{design choice}. 
Finally, this computed new reward is given to the agent that is unaware of this new mechanic. 
