\section{Reinforcement Learning} 

Reinforcement Learning is a method of learning that maps situations to
actions in order to maximize its rewards
\cite{sutton2018reinforcement}. Rewards are numerical values associated to a state and action. Precisely, one defines a reward function 
$R : (S \times A) \rightarrow \mathbb{R}$ where $S$ defines the state space and $A$ the action space. Note that a state refers to the current configuration
of the environment and the action refers to the action chosen by the RL agent. By defining this reward function and the scenario of the problem the agent is trying to solve, 
reinforcement learning has the advantage of not requiring a prior dataset. Indeed, the agent is not told what to do, but rather 
learns from the effect of its actions on the environment. 
% Consider Figure~\ref{fig:rl}. [GR] You are explaining the figure in
% the next paragraph, so this sentence is not needed.

\begin{figure}[H]
  \centering
  \includegraphics[scale=0.6]{figures/rlroutine.png}
  \caption{Reinforcement Learning Routine}
  \label{fig:rl}
\end{figure}


The diagram in Figure~\ref{fig:rl} is a high-level description of how
an agent using reinforcement learning can be trained.  
%
The upper left box represents the \emph{environment} as seen by the agent
according to its sensors.
%
The current state of the environment is represented as a \emph{state
vector}.
%
At each iteration, the agent will receive the state vector as input,
and needs to choose an \emph{action} to take.
%
Once the action is taken, the environment is updated to the next state
and the agent receives a \emph{reward} as feedback.
%
This reward is a domain dependent function that represents how
``good'' the new state is.
%
The agent's goal is to increase its reward by taking actions that
reach better states each time.
%
The triple (state, action, reward) helps the agent in shaping the
final policy.

\medskip 

The reward function is a crucial aspect of the RL algorithm. For instance, consider a game of chess 
where the agent is punished when it loses and rewarded if it wins. The agent is bound to learn how to 
maximize its winnings but it will need to exhaust multiple possible combinations to learn. In this case, 
the training time is not optimal. A better approach would be to also reward it for making a good opening, for instance. 
Another example would be only considering negative rewards. Say we want our agent to escape a maze, and we punish it at every timestep for not escaping. 
If there is a fatality state (\emph{e.g.}, a fire or a black whole), the agent will learn to move towards the fatality state as to cut its negative rewards as soon as possible. 
In conclusion, a good reward function is the first step of optimal learning. By choosing the right reward function, 
we can ensure a faster and more efficient training, possibly with fewer errors. 

For more information on the algorithms we have used for our implementations, refer to \ref{sec:rldetails}.

\subsection{Challenges in Reinforcement Learning}
\label{sec:challenges}

Reward shaping (1), the exploration-exploitation dilemma (2) and
meta-learning (3) are main challenges
that make it harder for RL to be adopted as a solution to more real-world problems. 

\medskip 

\emph{Reward shaping}~\cite{laud2011} refers to the lack of systematic methods to design a reward
  function that ensures fast and efficient learning. This generation of an appropriate 
reward function for a given problem is still an open challenge~\cite{kober2013}.
Ideally, rewards would be given by the real-world, i.e. \textit{native rewards}. For instance, recent work investigates dynamically generating a reward 
using a user verbal feedback to the autonomous agent~\cite{gonzalez2010}. However, most RL agents 
can only stay in simulation due to the lack of safety guarantees. This is because of the trial-and-error nature of the RL training. 
Thus, there exists a need for \textit{shaping rewards} instead. There are reasonable criteria on how this should be done, those are \emph{standard practices}. For instance, 
rewards that can be continuously harvested speed up convergence
compared to rewards that can only be harvested at ``the end''
(\emph{i.e.} the chess example). Similarly, one should avoid only 
negative rewards as that results in unwanted behavior. Furthermore, if dealing with a continuous state space, it helps to have a polynomial differential function as the reward function 
as it is shown to help the agent learn faster. Finally, one can normalize rewards at the end as to not end up with too many discrepancies. 
However, there still exists a lack of a systematic method to design a
reward function, and this needs to be done on a case by case basis.
%
\dio{} does not rely on an abstract representation to infer a reward function, rather only needs 
to care about the translation to a domain specific language, like prolog or datalog to assess a given world. 
It provides a more declarative approach to reason about rewards, thus providing a systematic method to map 
labels to rewards. As a consequence, it is able to handle (2), namely, the exploitation vs. exploration
dilemma.

\medskip

The \emph{exploration vs. exploitation dilemma} is the question of whether to always exploit what the
agent knows or explore in the hope that an unexplored state might
result in better rewards. This dilemma of \emph{exploration} vs. \emph{exploitation} is a central issue of RL. Consider this problem. An agent is at an intersection. It has the choice of going either right or left. 
It does not yet know the outcome of either. It chooses right at a given point and receives a reward $r=1$. The question is "When faced with the same decision, should it keep going right?" There are two issues to consider. 
First, it does not know the outcome of going left. It could be that there is a better reward waiting for it on the left lane. Second, when dealing with a stochastic environment, it might be that $r$ was a one-time occurence. 
It would be equivalent to someone buying a lottery ticket, and winning
\$1M on their first try, and thus, spending all that they won in trying to make it happen again. This problem showcases the importance of exploration; an agent 
needs to see where other paths might lead to, but also exploitation; if it keeps exploring forever it will never accumulate rewards. This is especially evident when the possible states cannot be exhausted. Several techniques have been proposed 
to balance between exploration and exploitation \cite{Kaelbling1996ReinforcementLA}. A notable one is the \emph{epsilon-greedy} technique. The idea is to set some probability $\epsilon$ by which the agent decides to explore. This probability can be adapted 
to decrease as more \emph{episodes} are completed. However, by ensuring (1), an informed reward function is able to sufficiently 
deter the exploration of undesirable states while encourage the exploitation of desirable ones, continuously adapting to 
acquired knowledge and resolving the conflict when necessary. More interestingly, a solution to (2) impacts (3). 


\medskip

\emph{Meta-learning} is the problem of deploying an agent trained in a simulation to
the real-world, or possibly another simulation, where it encounters
state configurations it did not during its training. The goal is to
be able to efficiently adapt to those configurations. The problem of meta-learning in RL stems from the uncertainties of the world. 
Consider the result of training: a function $\pi$ that map states to
actions $\pi : S \rightarrow A$. The learned policy is the one that maximizes the cumulative rewards.
This training is most often done in simulation, given the lack of safety guarantees of RL. 
However, several problems come into place when considering the deployment of the trained agent. Considering that an agent has done well in 
a designated simulation does not imply that it will do as well in the real-world. Overall, it must be that certain uncertainties will not be expected, thus there can be no expectation on how the agent will behave 
when out of simulation. Meta-learning in reinforcement learning is the problem of learning-to-learn, which is about efficiently
adapting a learned policy to conditions and tasks that were not encountered in the past. In RL, meta-learning
involves adapting the learning parameters, balancing exploration and exploitation to direct the
agent interaction \cite{gupta_meta-reinforcement_2018,schweighofer_meta-learning_2003}. Meta-learning is a central problem in AI, since an agent that can solve more
and more problems it has not seen before, approaches the ideal of a general-purpose AI. However, as noted previously, a solution to (2) implies 
a continuous adaptation to knowledge. Since the conflict of exploration and exploitation is resolved, the agent adapts accordingly to tasks it encountered in the past (exploiting), but also 
tasks it encounters for the first time (exploring). Thus, from (2) one
can have a significant impact on (3).
