\section{Dynamic Obstacles in a GridWorld : Working Example} 
\label{gridworlddyn}

For our experiments, we take the following scenario. An agent exists in a 100x100 grid. 
There are $n$ dynamic obstacles around that have uniform chances of choosing any direction to move in. 
The agent can make a decision to move either right, left, top or bottom, as well as not moving at all. 
There is a goal in the bottom-right of the grid. The agent has two goals: (1) survive by not crashing with the obstacles and (2) reach the goal in the lowest number of steps possible. 

\subsection{Scenario in Reinforcement Learning}

This environment offered by gym-gridworld~\cite{gym_minigrid} is useful for testing our algorithm in a Dynamic Obstacle avoidance for a partially observable 
environment. Precisely, we define the state as follows. 
\begin{equation*}
  S_t = [x, y, d, G]
\end{equation*}
$(x,y)$ define the position of our agent while $d$ its direction. $G$
is the gridworld observed by the agent which includes walls, obstacles
and free squares. Note that this observed environment is from the point of the view 
of the agent and does not represent the entire grid. 
The action space is, 
\begin{equation*}
  A_t = \{ right: 0, up: 1, down: 2, left: 3 \}
\end{equation*}
Finally, the reward is a straightforward one that reward $1$ for reaching the goal, $-1$ for failing to do so, either by colliding with an obstacle, 
or exhausting its battery (maximum number of steps). Furthermore, for every step the agent takes, it has to spend the 'cost' of living, which is $-\dfrac{1}{n}$, where $n$ is the size of board.


\subsection{Domain Specific Rules}
The rules are defined as a ProbLog~\cite{problog}: a probabilistic prolog that allows us to capture 
the stochasticity of the environment, as we previously introduced in \ref{sec:problog}. Precisely, we want to consider the erratic movements of the obstacles, considering 
we do not have previous knowledge on the distribution of their given movement. We assume a uniform distribution and define the following. 
The rules of \dio{} deriving a predicate $\gamma$ take the following form: 

\begin{prooftree}
  \AxiomC{$P_1 :: \psi_1$}
  \AxiomC{\dots}
  \AxiomC{$P_n :: \psi_n$}
  %\AxiomC{$\sum_{i=0}^{n}P(i) = 1$}
  \RightLabel{(action)}
  \TrinaryInfC{$Q :: \gamma$}
\end{prooftree}

\noindent
where $Q$ and $P_1, ..., P_n$ are probabilities for the facts $\gamma$
and $\psi_1, ..., \psi_n$, respectively.
%
The rule should be read as an implication from the top down, i.e., if
the premise facts $\psi_1, ..., \psi_n$ hold with their corresponding
probabilities, than we can infer the conclusion $\gamma$ with
probability $Q$.
%
Note that $Q$ will naturally be a function of $P_1, ..., P_n$.
%
If there are $k$ rules with conclusions $Q_1 :: \gamma, ..., Q_k ::
\gamma$, then it must be the case that $\sum_{i=0}^{k} Q_i = 1$.
%, and $\varphi(i)$ corresponds to the conjunction of grounds facts of the possible world with probability $P_i$.
The action is equivalent to our step semantics, thus, we enforce that a given action modifies the facts in some form. In practice, an action is the missing 
clause to generate the next predicate. In the gridworld example, we give the following. 

\vspace{0.2cm}
\begin{center}
  \AxiomC{atPos(X,Y)}
  \AxiomC{speed(V)}
  \AxiomC{timestep(T)}
  \RightLabel{(right)}
  \LeftLabel{(1)}
  \TrinaryInfC{atPos(X + V*T, Y)}
  \DisplayProof
  \hspace{0.2cm}
  %
  \AxiomC{obs(X,Y,V)}
  \AxiomC{timestep(T)}
  \LeftLabel{(2)}
  \RightLabel{(time)}
  \BinaryInfC{0.25 :: obs(X + V*T, Y, V)}
  \DisplayProof
\end{center}
\vspace{0.2cm}

(1) considers the movement of the agent while (2) considers the movement of the obstacles. Note that (2) considers 
a uniform distribution over the movement of the obstacle, since every obstacle has a uniform probability of moving up/down/left/right. 
We could do the same for (1) by considering the probability of an action failing. In our case, we assume the movement is deterministic and no failure over the movement 
of the agent happens.


\subsection{World Knowledge}
Our world knowledge base covers the agent, the obstacles and the timestep. We consider two cases: 
\textit{constant} ground facts vs. \textit{dynamic} ground facts. The latter represents positions which are dynamically 
generated at every timestep while the former considers only the facts that remain true in every world, thus include the timestep, since we always
move by 1-unit, and the speed, since the agent and the obtacles are defined to only move by 1-box every time. Given that our knowledge base $Kb$ is defined by, 
\[
    C = \{speed(1), timestep(1) \}     
    \qquad
    D = \{atPos(X, Y), obs(X,Y,1)\}
    \qquad
    Kb = C \cup D
\]

\subsection{From Norms to Labels}
In the following, we define \textbf{\glsplural{norms}} as textual sentences to describe the intended goal behavior. 
Thus, in the gridworld example, we consider one possible norm, (1) \textit{crash} when the agent and the obstacle \emph{possibly} overlap. That is if there is a chance of 
the obstacle and the agent choosing to move to the same square. 

\[
    \infer{P_1 \times P_2 :: maybecrash}{P_1::atPos(X,Y), P_2::obs(X,Y,\_)
      & \ldots}
\] 

Note that the final $P$ associated to our label, is precisely $Pr[L \mid (s_t, a_t)]$. 

\subsection{Translation Unit}
The translation unit is the bridge between \dio{} and RL. Precisely, it handles both feeding 
the world facts to \dio{} and translates the feedback given to a numerical value, through 
a given function. First, the \dio{}/RL loop is given in Algorithm 1. 

  \begin{algorithm}[H]
    \caption{\dio{}/RL Loop}
    \begin{algorithmic}[1]
    
    \Procedure{Step}{$S_t, a$}       \Comment{(State, action) Pair}
        \State Check for invalid actions
        \State Check for obstacles 
        \State Update Obstacles positions
        \State Update Agent's position
        \State \textbf{TU.UpdateWorld}(position, direction, obstacles) \Comment{KB update in \dio{}}
        \State $obs, r_{rl}$ = Step'$(S_t, ac)$ \Comment{Call to Initial Env}
        \State $r_{\dio{}}'$ = \textbf{getFeedback()} \Comment{Query \dio{}}
        \State $R$ = TU.getReward($r_{rl}, r_{\dio{}}'$) 
        \State return (obs, R)
    \EndProcedure
    
    \end{algorithmic}
    \end{algorithm}

Note that the translation unit first updates the knowledge base from \dio{} side. 
Given this update, it is possible to query \dio{} over the labels and their associated probabilities. Precisely, 
the \emph{getFeedback} procedure computes $r'_{\dio{}}$ the expectated value over the possible worlds. 

\begin{algorithm}[H]
  \caption{Inference of Judgment $r'_{\dio{}}$}
  \begin{algorithmic}[1]
      
      \Procedure{getFeedback}{} 
      \State labels $\gets$ \textbf{getLabels()} \Comment{Labels - Associated Probabilities}
      \State $P \gets \{$crash$\}$  \Comment{Set of possible worlds}
      \State $I \gets \{-1\}$ \Comment{Indicators associated with world}
      \State $i \gets 1$
      \State $r \gets 0$
      \While{$i \leq$ length(labels)} 
         \State $r \gets r +$ labels[P[i]]*F[i] \Comment{Equivalent to the Expectation}
         \State $i \gets i+1$

      \EndWhile
      
      \EndProcedure
      
  \end{algorithmic}
  \end{algorithm}

  Finally, the translation unit \textbf{TU} can compute the final reward $r'_{\dio{}}$ by normalizing it given the number of states and multiply it by a chosen $\alpha$, before adding it to the original reward $r_{rl}$. 

