\section{Related Work}
%
%% Our inspiration : Neurosymbolic AI! What is it?
%
To tackle the reward shaping challenge from Section~\ref{sec:challenges}, we are
inspired by the current Neurosymbolic AI trends, which explore
combinations of deep learning (DL) and symbolic reasoning.
%
The work has been a response to criticism on DL's lack of formal
semantics and intuitive explanation, and the lack of expert knowledge
towards guiding machine learning models.

%
%% How do they incorporate domain knowledge?
%
Current Neurosymbolic AI trends are concerned with knowledge representation and reasoning, namely, they investigate computational-logic systems 
and representation to precede learning in order to provide some form
of incremental update, e.g. a meta-network to group two sub-neural
networks~\cite{Besold2017NeuralSymbolicLA}.
As a result, neurosymbolic AI has been successfully applied to vision-based tasks such as semantic labeling \cite{vinyals2015, karpathy2015}, 
vision analogy-making \cite{Reed2015DeepVA}, or learning communication
protocols \cite{Foerster2016LearningTC}.
%
%% When they do, there are promising results in machine learning.
%
In general, neurosymbolic AI trends show promising results in improving ML algorithms, whether that is from 
an interpretability aspect or an optimization one. More recent works take this trend and incorporate symbolic reasoning and 
domain knowledge in reinforcement learning settings \cite{Driessens2010,Romero2020,achiam2017,marek2010}. \cite{marek2010,Romero2020} use the general idea of \textit{reward shaping} and \textit{epsilon adaptation} respectively 
to incorporate procedural knowledge into a RL algorithm. 
Both works introduce this combination as a successful strategy to guide the exploration and exploitation tradeoff in RL. They both show promising results. While 
\cite{marek2010} focuses on providing formal specifications for reward shaping, it lacks practical 
consequences to the implementation of most RL to make use of its formal methods conclusions. On the other hand, \cite{Romero2020} proposes a method to adapt $\epsilon$ based on domain knowledge, the method is specifically applied to "Welding Sequence Optimization".  
To do so, the RL algorithm is modified in itself, similarly to what was done in \cite{Driessens2010}. Precisely, in \cite{Driessens2010}, the RL algorithm itself is 
modified to deal with states that are model-based as opposed to vectors. They defined their method as Relational RL. 
Furthermore, they conclude that by using more expressive representation language for the RL scenario, their method can potentially offer a solution to the problem of meta-learning. 
While \cite{Romero2020,Driessens2010} both present promising rewards, they lack the modularity necessary for scaling the proposed methods to further RL implementations. 

%
%% But this is ALSO the case for reinforcement learning specifically: reward machines
%
This is further reinforced by more recent work, precisely, \emph{reward machines} that define an automaton to adapt a reward function given 
step transitions~\cite{icarte2022reward}. By exposing the structure in the reward function, \cite{icarte2022reward} shows that this enables to find solutions faster. 
%
%% This is great! But there's an issue with reward machines..
%
However, given the nature of a state machine, reward machines are unable to adapt to the uncertainties of the world. 

%
%% And thus, we provide dio..
%
To face those limitations, \dio{} does not rely on an abstract representation to infer a reward function, rather only needs 
to care about the translation to a domain specific language, like prolog or datalog to assess a given world. 
The stochasticity of the world is then inherent, given a probabilistic logic program.
\dio{} provides a more declarative approach to reason about rewards, thus providing a systematic method to map 
labels to rewards. 

