%%
%% This is file `sample-acmsmall-conf.tex',
%% generated with the docstrip utility.
%%
%% The original source files were:
%%
%% samples.dtx  (with options: `acmsmall-conf')
%% 
%% IMPORTANT NOTICE:
%% 
%% For the copyright see the source file.
%% 
%% Any modified versions of this file must be renamed
%% with new filenames distinct from sample-acmsmall-conf.tex.
%% 
%% For distribution of the original source see the terms
%% for copying and modification in the file samples.dtx.
%% 
%% This generated file may be distributed as long as the
%% original source files, as listed above, are part of the
%% same distribution. (The sources need not necessarily be
%% in the same archive or directory.)
%%
%%
%% Commands for TeXCount
%TC:macro \cite [option:text,text]
%TC:macro \citep [option:text,text]
%TC:macro \citet [option:text,text]
%TC:envir table 0 1
%TC:envir table* 0 1
%TC:envir tabular [ignore] word
%TC:envir displaymath 0 word
%TC:envir math 0 word
%TC:envir comment 0 0
%%
%%
%% The first command in your LaTeX source must be the \documentclass
%% command.
%%
%% For submission and review of your manuscript please change the
%% command to \documentclass[manuscript, screen, review]{acmart}.
%%
%% When submitting camera ready or to TAPS, please change the command
%% to \documentclass[sigconf]{acmart} or whichever template is required
%% for your publication.
%%
%%
\documentclass[acmsmall]{acmart}

%%
%% \BibTeX command to typeset BibTeX logo in the docs
\AtBeginDocument{%
  \providecommand\BibTeX{{%
    Bib\TeX}}}


%%
%% Submission ID.
%% Use this when submitting an article to a sponsored event. You'll
%% receive a unique submission ID from the organizers
%% of the event, and this ID should be used as the parameter to this command.
%%\acmSubmissionID{123-A56-BU3}

%%
%% For managing citations, it is recommended to use bibliography
%% files in BibTeX format.
%%
%% You can then either use BibTeX with the ACM-Reference-Format style,
%% or BibLaTeX with the acmnumeric or acmauthoryear sytles, that include
%% support for advanced citation of software artefact from the
%% biblatex-software package, also separately available on CTAN.
%%
%% Look at the sample-*-biblatex.tex files for templates showcasing
%% the biblatex styles.
%%

%%
%% The majority of ACM publications use numbered citations and
%% references.  The command \citestyle{authoryear} switches to the
%% "author year" style.
%%
%% If you are preparing content for an event
%% sponsored by ACM SIGGRAPH, you must use the "author year" style of
%% citations and references.
%% Uncommenting
%% the next command will enable that style.
%%\citestyle{acmauthoryear}


\usepackage{bussproofs}
\usepackage{proof}
\usepackage{algorithm}
\usepackage[noend]{algpseudocode}
\usepackage[toc]{glossaries}
\usepackage{glossary-mcols}
\usepackage{soul}

\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]
\loadglsentries{terminology}

% For comment boxes.
\usepackage[colorinlistoftodos,prependcaption,textsize=tiny]{todonotes}
\newcommand{\giselle}[1]{\todo[linecolor=red,backgroundcolor=red!25,bordercolor=red]{G: #1}}
\newcommand{\samar}[1]{\todo[linecolor=blue,backgroundcolor=blue!25,bordercolor=blue]{S: #1}}
\newcommand{\dio}{\textsc{dio}}

%%
%% end of the preamble, start of the body of the document source.
\begin{document}


%%
%% The "title" command has an optional parameter,
%% allowing the author to define a "short title" to be used in page headers.
\title{Domain Informed Oracle for Reinforcement Learning}

%%
%% The "author" command and its associated commands are used to define
%% the authors and their affiliations.
%% Of note is the shared affiliation of the first two authors, and the
%% "authornote" and "authornotemark" commands
%% used to denote shared contribution to the research.
\author{Samar Rahmouni}
\email{srahmoun@andrew.cmu.edu}
\orcid{0000-0003-1351-1515}
\affiliation{%
  \institution{Carnegie Mellon University}
  \city{Doha}
  \country{Qatar}
}

\author{Giselle Reis}
\email{giselle@cmu.edu}
\orcid{0000-0002-5145-9829}
\affiliation{%
  \institution{Carnegie Mellon University}
  \city{Doha}
  \country{Qatar}
}


%% By default, the full list of authors will be used in the page
%% headers. Often, this list is too long, and will overlap
%% other information printed in the page headers. This command allows
%% the author to define a more concise list
%% of authors' names for this purpose.
%\renewcommand{\shortauthors}{Trovato et al.}


%%
%% The abstract is a short summary of the work to be presented in the
%% article.
\begin{abstract}
  %% The story

  %% There is this great new method!
  Reinforcement learning (RL) is a powerful AI
  technique that does not
  require pre-gathered data but relies on a trial-and-error process
  for the agent to learn. 
  %
  This is made possible through a reward function that associates
  state configurations to a numerical value. 
  %
  The agent's goal is to maximize its cumulative reward over its
  lifetime. 

  %% Oh, but there are some issues...
  Unfortunately, there is no systematic method to design a reward
  function since interpreting abstract states in RL in
  the context of a domain needs to be done on a case by case basis.

  %% This issue is crucial [motivation]
  \textcolor{red}{Summary of motivation.}
  
  %% We can fix that!
  We propose a \emph{Domain Informed Oracle (\dio{})} to
  systematically incorporate domain specific knowledge into RL
  reward functions.
  %
  \dio{} is a collection of domain specific rules written in a
  declarative language, such as Prolog.
  %
  It does not rely on the RL representation of states, allowing the
  programmer to focus on the domain knowledge using an
  expressive and intuitive language, where states and rules can
  be defined conveniently.
  %
  For each state and action pair, \dio{} provides
  information to the reward
  function, to dynamically adapt itself.
  %
  %% Methodology
  Our implementation is tested on a grid world with dynamic obstacles.
  and compared to a basic RL algorithm. 
  %
  %% Results and Conclusions
  \textcolor{red}{ADD results and conclusions.}
  

\end{abstract}

%%
%% The code below is generated by the tool at http://dl.acm.org/ccs.cfm.
%% Please copy and paste the code instead of the example below.
%%
\begin{CCSXML}
  <ccs2012>
     <concept>
         <concept_id>10003752.10003790.10003795</concept_id>
         <concept_desc>Theory of computation~Constraint and logic programming</concept_desc>
         <concept_significance>300</concept_significance>
         </concept>
   </ccs2012>
\end{CCSXML}
  
\ccsdesc[300]{Theory of computation~Constraint and logic programming}

%%
%% Keywords. The author(s) should pick words that accurately describe
%% the work being presented. Separate the keywords with commas.
\keywords{probabilistic logic programming, reinforcement learning, reward shaping}
%% A "teaser" image appears between the author and affiliation
%% information and the body of the document, and typically spans the
%% page.

%%\received{20 February 2007}
%%\received[revised]{12 March 2009}
%%\received[accepted]{5 June 2009}

%%
%% This command processes the author and affiliation and title
%% information and builds the first part of the formatted document.
\maketitle

\section{Introduction}

Implementing a robust adaptive controller that is effective in terms
of precision, time, and quality of decision in dynamic and uncertain
scenarios has always been a central challenge in AI and robotics. When
autonomous agents are deployed in the real world, we want to ensure
that they are able to adapt to unforeseen scenarios, as well as keep
their efficiency. This efficiency is measured in terms of optimality
of actions and time to make a decision. 
%
Since we are unable to
provide a repertoire of all possible scenarios and actions, agents
need to be able to autonomously predict and adapt to changes.
Reinforcement Learning (RL) is an approach that supports dynamically
adapting to new input~\cite{sutton2018reinforcement}. It has beed
successfully used by AlphaGo, Deepmind AlphaStar, and OpenAI Five to
solve Go, StarCraft II and Dota 2~\cite{li2019reinforcement}.
%
Reinforcement Learning is a powerful tool as it does not require
pre-gathered data as most Machine Learning (ML) techniques do. 
%
The general idea of RL is learning via trial-and-error, guided by a
\textit{domain dependent} reward function.
%
For example, if the agent is a self-driving car, the reward function
would greatly penalize states where it crashes.
%
However, this means that the car is bound to crash to learn not to
crash again.
%
A better reward function can include the physics equations to predict,
with some degree of certainty, the car's trajectory for the next few
seconds.
%
By looking into the future, the reward function can penalize bad
behavior before it reaches a catastrophic state (a crash).
%
A better reward function prunes the (often infinite) search space
faster, allowing the agent to explore (breadth) new states instead of
exploiting (depth) dead ends.
%
The task of choosing the reward function is thus crucial, yet difficult as shown in \ref{sec:challenges}.\samar{Adding a reference to motivation instead of emphasizing here.}
%
In this work, we propose a Domain Informed Oracle (\dio{}) written in a
declarative language to inform a reinforcement learning algorithm. 
%
Our method provides a systematic way to encode domain specific rules
into a reward function for RL that does not rely on the state
representation within the RL algorithm.

%%
%% Our motivation from the lack of symbolic reasoning
%% to inform a reinforcement learning modules
%% The challenges of finding a 'good' reward function
%%
\input{motivation.tex}

%%
%% What is it that we propose? 
%% Domain Informed Oracle! A logic programming module to 
%% inform a reinforcement learning module through reward-shaping
%%
\input{dio.tex}

%%
%% How would this look in practice?
%% Let's go through our working example. 
%%
\input{gridworld.tex}


%%
%% Methodology to include our metrics of safety and optimality
%% During both training and deployement
%%
\input{methodology.tex}

%%
%% Results given alpha and obstacles settings 
%%
\section{Results}
\textcolor{red}{The FINAL PART AAAA.}

%% 
%% Related work 
%%
\input{relatedwork.tex} 

\section{Conclusions}

In conclusion, as RL faces the issues of reward shaping, meta-learning and the exploration-exploitation dilemma, domain knowledge show promising results in 
improving reinforcement learning methods. The main challenge is to make such an integration seamless, and independent of the AI implementation. 
This is a task we were able to produce in our simpler introductary example of the Dynamic Obstacles in the grid world. Results are promising and will be further 
extended to the traffic simulation referred to in \ref{traffic}. More directions open up as we think of optimization, this includes mix-matching the n-steps approach with the 
number of steps \dio{} can look ahead. Similarly, we look at the differences between overwriting vs. fine-tuning the rewards using \dio{} and if such choice matters in training. 
As we start adding complexity to the algorithm, we turn our focus into the specifications as shown in \ref{scspecs} to better inform 
on meaningful and effective way to translate our labels to their corresponding numerical values design choice. 


%%
%% The acknowledgments section is defined using the "acks" environment
%% (and NOT an unnumbered section). This ensures the proper
%% identification of the section in the article metadata, and the
%% consistent spelling of the heading.

% TODO: acknowledge reviewers
%\begin{acks}
%To Robert, for the bagels and explaining CMYK and color spaces.
%\end{acks}

%%
%% The next two lines define the bibliography style to be used, and
%% the bibliography file.
\bibliographystyle{ACM-Reference-Format}
\bibliography{biblio.bib}


%%
%% If your work has an appendix, this is the place to put it.
%\appendix


\end{document}
%\endinput
%%
%% End of file `main.tex'.
