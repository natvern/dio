@misc{riegel2020logical,
      title={Logical Neural Networks}, 
      author={Ryan Riegel and Alexander Gray and Francois Luus and Naweed Khan and Ndivhuwo Makondo and Ismail Yunus Akhalwaya and Haifeng Qian and Ronald Fagin and Francisco Barahona and Udit Sharma and Shajith Ikbal and Hima Karanam and Sumit Neelam and Ankita Likhyani and Santosh Srivastava},
      year={2020},
      eprint={2006.13155},
      archivePrefix={arXiv},
      primaryClass={cs.AI}
}

@misc{garcez2020neurosymbolic,
      title={Neurosymbolic AI: The 3rd Wave}, 
      author={Artur d'Avila Garcez and Luis C. Lamb},
      year={2020},
      eprint={2012.05876},
      archivePrefix={arXiv},
      primaryClass={cs.AI}
}

@INPROCEEDINGS{vnc20, 
      author={Dantas, Yuri Gil and Nigam, Vivek and Talcott, Carolyn},  
      booktitle={2020 IEEE Vehicular Networking Conference (VNC)},   
      title={A Formal Security Assessment Framework for Cooperative Adaptive Cruise Control},   
      year={2020},  
      volume={},  
      number={},  
      pages={1-8},  
      doi={10.1109/VNC51378.2020.9318334}}

@article{Sarker2021NeuroSymbolicAI,
  title={Neuro-Symbolic Artificial Intelligence: Current Trends},
  author={Md. Kamruzzaman Sarker and Lu Zhou and Aaron Eberhart and P. Hitzler},
  journal={ArXiv},
  year={2021},
  volume={abs/2105.05330}
}

@article{schweighofer_meta-learning_2003,
	title = {Meta-learning in {Reinforcement} {Learning}},
	volume = {16},
	OPTurl = {https://linkinghub.elsevier.com/retrieve/pii/S0893608002002289},
	OPTdoi = {10.1016/S0893-6080(02)00228-9},
	abstract = {Meta-parameters in reinforcement learning should be tuned to the environmental dynamics and the animal performance. Here, we propose a biologically plausible meta-reinforcement learning algorithm for tuning these meta-parameters in a dynamic, adaptive manner. We tested our algorithm in both a simulation of a Markov decision task and in a non-linear control task. Our results show that the algorithm robustly ﬁnds appropriate meta-parameter values, and controls the meta-parameter time course, in both static and dynamic environments. We suggest that the phasic and tonic components of dopamine neuron ﬁring can encode the signal required for meta-learning of reinforcement learning. q 2002 Elsevier Science Ltd. All rights reserved.},
	language = {en},
	number = {1},
	journal = {Neural Networks},
	author = {Schweighofer, Nicolas and Doya, Kenji},
	month = jan,
	year = {2003},
	pages = {5--9},
}

@article{Garca2015ACS,
  title={A comprehensive survey on safe reinforcement learning},
  author={Javier Garc{\'i}a and F. Fern{\'a}ndez},
  journal={J. Mach. Learn. Res.},
  year={2015},
  volume={16},
  pages={1437-1480}
}

@article{law2005,
author = {Law, Edith},
year = {2005},
month = {01},
pages = {},
title = {Risk-directed exploration in reinforcement learning}
}

@inproceedings{gupta_meta-reinforcement_2018,
	title = {Meta-{Reinforcement} {Learning} of {Structured} {Exploration} {Strategies}},
	abstract = {Exploration is a fundamental challenge in reinforcement learning (RL). Many current exploration methods for deep RL use task-agnostic objectives, such as information gain or bonuses based on state visitation. However, many practical applications of RL involve learning more than a single task, and prior tasks can be used to inform how exploration should be performed in new tasks. In this work, we study how prior tasks can inform an agent about how to explore effectively in new situations. We introduce a novel gradient-based fast adaptation algorithm – model agnostic exploration with structured noise (MAESN) – to learn exploration strategies from prior experience. The prior experience is used both to initialize a policy and to acquire a latent exploration space that can inject structured stochasticity into a policy, producing exploration strategies that are informed by prior knowledge and are more effective than random action-space noise. We show that MAESN is more effective at learning exploration strategies when compared to prior meta-RL methods, RL without learned exploration strategies, and task-agnostic exploration methods. We evaluate our method on a variety of simulated tasks: locomotion with a wheeled robot, locomotion with a quadrupedal walker, and object manipulation.},
	language = {en},
	booktitle = {Conference and {Workshop} on {Neural} {Information} {Processing} {Systems} ({NeurIPS})},
	author = {Gupta, Abhishek and Mendonca, Russell and Liu, YuXuan and Abbeel, Pieter and Levine, Sergey},
	year = {2018},
	pages = {10},
}


@article{Besold2017NeuralSymbolicLA,
  title={Neural-Symbolic Learning and Reasoning: A Survey and Interpretation},
  author={Tarek R. Besold and A. Garcez and Sebastian Bader and H. Bowman and Pedro M. Domingos and P. Hitzler and Kai-Uwe K{\"u}hnberger and L. Lamb and Daniel Lowd and P. Lima and L. Penning and Gadi Pinkas and Hoifung Poon and Gerson Zaverucha},
  journal={ArXiv},
  year={2017},
  volume={abs/1711.03902}
}

@inproceedings{vinyals2015,
author = {Vinyals, Oriol and Toshev, Alexander and Bengio, Samy and Erhan, Dumitru},
year = {2015},
month = {06},
pages = {3156-3164},
title = {Show and tell: A neural image caption generator},
doi = {10.1109/CVPR.2015.7298935}
}

@inproceedings{karpathy2015,
author = {Karpathy, Andrej and Li, Fei},
year = {2015},
month = {06},
pages = {3128-3137},
title = {Deep visual-semantic alignments for generating image descriptions},
doi = {10.1109/CVPR.2015.7298932}
}

@inproceedings{Reed2015DeepVA,
  title={Deep Visual Analogy-Making},
  author={Scott E. Reed and Yi Zhang and Y. Zhang and Honglak Lee},
  booktitle={NIPS},
  year={2015}
}

@article{Foerster2016LearningTC,
  title={Learning to Communicate to Solve Riddles with Deep Distributed Recurrent Q-Networks},
  author={Jakob N. Foerster and Yannis M. Assael and N. D. Freitas and S. Whiteson},
  journal={ArXiv},
  year={2016},
  volume={abs/1602.02672}
}

@techreport{li2019reinforcement,
	title = {Reinforcement {Learning} {Applications}},
	url = {http://arxiv.org/abs/1908.06973},
	abstract = {We start with a brief introduction to reinforcement learning (RL), about its successful stories, basics, an example, issues, the ICML 2019 Workshop on RL for Real Life, how to use it, study material and an outlook. Then we discuss a selection of RL applications, including recommender systems, computer systems, energy, finance, healthcare, robotics, and transportation.},
	number = {arXiv:1908.06973},
	author = {Li, Yuxi},
	month = aug,
	year = {2019},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
	file = {Li - 2019 - Reinforcement Learning Applications.pdf:/Users/giannidicaro/Zotero/storage/4Y4W8K55/Li - 2019 - Reinforcement Learning Applications.pdf:application/pdf}
}

@article{rockafellar2000,
author = {Rockafellar, R and Uryasev, Stan},
year = {2000},
month = {01},
pages = {21-42},
title = {Optimization of Conditional Value-At-Risk},
volume = {2},
journal = {Journal of risk}
}

@article{Siebel2007EvolutionaryRL,
  title={Evolutionary reinforcement learning of artificial neural networks},
  author={Nils T. Siebel and G. Sommer},
  journal={Int. J. Hybrid Intell. Syst.},
  year={2007},
  volume={4},
  pages={171-183}
}

@inproceedings{Li2021SafeRL,
  title={Safe Reinforcement Learning Using Robust Action Governor},
  author={Yutong Li and N. Li and H. E. Tseng and A. Girard and Dimitar Filev and I. Kolmanovsky},
  booktitle={L4DC},
  year={2021}
}

@article{alshiekh2017,
author = {Alshiekh, Mohammed and Bloem, Roderick and Ehlers, Rüdiger and Könighofer, Bettina and Niekum, Scott and Topcu, Ufuk},
year = {2017},
month = {08},
pages = {},
title = {Safe Reinforcement Learning via Shielding}
}

@article{urban2021,
  title={A Review of Formal Methods applied to Machine Learning},
  author={Caterina Urban and Antoine Min'e},
  journal={ArXiv},
  year={2021},
  volume={abs/2104.02466}
}

@inproceedings{kurdkelly2003,
author = {Kurd, Zeshan and Kelly, Tim},
year = {2003},
month = {09},
pages = {163-169},
title = {Establishing Safety Criteria for Artificial Neural Networks},
volume = {2773},
isbn = {978-3-540-40803-1},
doi = {10.1007/978-3-540-45224-9_24}
}

@article{LarssonErik2015Tvpp,
copyright = {2015 Elsevier Ltd},
issn = {0968-090X},
abstract = {‚Ä¢Develops a framework for modeling platooning vehicles traveling in road networks.‚Ä¢Defines the vehicle platooning problem and proves finding its optimum is NP-hard.‚Ä¢Presents heuristics that can solve large instances of the platooning problem.
We create a mathematical framework for modeling trucks traveling in road networks, and we define a routing problem called the platooning problem. We prove that this problem is NP-hard, even when the graph used to represent the road network is planar. We present integer linear programming formulations for instances of the platooning problem where deadlines are discarded, which we call the unlimited platooning problem. These allow us to calculate fuel-optimal solutions to the platooning problem for large-scale, real-world examples. The problems solved are orders of magnitude larger than problems previously solved exactly in the literature. We present several heuristics and compare their performance with the optimal solutions on the German Autobahn road network. The proposed heuristics find optimal or near-optimal solutions in most of the problem instances considered, especially when a final local search is applied. Assuming a fuel reduction factor of 10% from platooning, we find fuel savings from platooning of 1‚Äì2% for as few as 10 trucks in the road network; the percentage of savings increases with the number of trucks. If all trucks start at the same point, savings of up to 9% are obtained for only 200 trucks.},
journal = {Transportation research. Part C, Emerging technologies},
author = {Larsson, Erik and Sennton, Gustav and Larson, Jeffrey},
address = {OXFORD},
keywords = {Analysis ; Ber√§kningsmatematik ; Computational complexity ; Computational Mathematics ; Computer science ; Matematik ; Mathematics ; Natural Sciences ; Naturvetenskap ; Science & Technology ; Technology ; Transportation ; Transportation Science & Technology ; Vehicle platooning ; Vehicle routing},
language = {eng},
number = {C},
organization = {Argonne National Lab. (ANL), Argonne, IL (United States)},
pages = {258-277},
publisher = {Elsevier India Pvt Ltd},
title = {The vehicle platooning problem: Computational complexity and heuristics},
volume = {60},
year = {2015},
}


@book{sutton2018reinforcement,
  title={Reinforcement learning: An introduction},
  author={Sutton, Richard S and Barto, Andrew G},
  year={2018},
  publisher={MIT press}
}

@article{laud2011,
author = {Laud, Adam},
year = {2011},
month = {04},
pages = {},
title = {Theory and Application of Reward Shaping in Reinforcement Learning}
}

@inproceedings{gonzalez2010,
author = {Tenorio-González, Ana and Morales, Eduardo and Villaseñor-Pineda, Luis},
year = {2010},
month = {11},
pages = {483-492},
title = {Dynamic Reward Shaping: Training a Robot by Voice},
isbn = {978-3-642-16951-9},
doi = {10.1007/978-3-642-16952-6_49}
}

@article{kober2013,
author = {Kober, Jens and Bagnell, J. and Peters, Jan},
year = {2013},
month = {09},
pages = {1238-1274},
title = {Reinforcement Learning in Robotics: A Survey},
volume = {32},
isbn = {978-3-642-27644-6},
journal = {The International Journal of Robotics Research},
doi = {10.1177/0278364913495721}
}

@article{Kaelbling1996ReinforcementLA,
  title={Reinforcement Learning: A Survey},
  author={Leslie Pack Kaelbling and Michael L. Littman and Andrew W. Moore},
  journal={J. Artif. Intell. Res.},
  year={1996},
  volume={4},
  pages={237-285}
}

@inproceedings{Dosovitskiy17,
  title = { {CARLA}: {An} Open Urban Driving Simulator},
  author = {Alexey Dosovitskiy and German Ros and Felipe Codevilla and Antonio Lopez and Vladlen Koltun},
  booktitle = {Proceedings of the 1st Annual Conference on Robot Learning},
  pages = {1--16},
  year = {2017}
}

@misc{tekol2020,
  author = {Tekol, Yüce},
  title = {PySwip},
  year = {2020},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/yuce/pyswip}},
  commit = {104132e8f8376dbb1c46952d414aa138ee58f6e5}
}

@Inbook{Driessens2010,
author="Driessens, Kurt",
editor="Sammut, Claude
and Webb, Geoffrey I.",
title="Relational Reinforcement Learning",
bookTitle="Encyclopedia of Machine Learning",
year="2010",
publisher="Springer US",
address="Boston, MA",
pages="857--862",
isbn="978-0-387-30164-8",
doi="10.1007/978-0-387-30164-8_721",
url="https://doi.org/10.1007/978-0-387-30164-8_721"
}

@article{Romero2020,
title = {Incorporating domain knowledge into reinforcement learning to expedite welding sequence optimization},
journal = {Engineering Applications of Artificial Intelligence},
volume = {91},
pages = {103612},
year = {2020},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2020.103612},
url = {https://www.sciencedirect.com/science/article/pii/S0952197620300804},
author = {Jesus Romero-Hdz and Baidya Nath Saha and Seiichiro Tstutsumi and Riccardo Fincato},
keywords = {Welding sequence optimization, FEA based welding simulation, Reinforcement learning, Structural deformation, Residual stress, Artificial intelligence, Machine learning},
abstract = {Welding Sequence Optimization (WSO) is very effective to minimize the structural deformation, however selecting proper welding sequence leads to a combinatorial optimization problem. State-of-the-art algorithms could take more than one week to compute the best sequence for an assembly of eight weld beads which is unrealistic for the early stages of Product Delivery Process (PDP). In this article, we develop and implement a novel Reinforcement Q-learning algorithm for WSO where structural deformation is used to compute reward function. We utilize a thermo-mechanical Finite Element Analysis (FEA) to predict deformation. The exploration–exploitation dilemma has been tackled by domain knowledge driven ε-greedy algorithm into Q-RL which helps to expedite the WSO and we call this novel algorithm as DKQRL. We run welding simulation experiment using well-known Simufact® software on a typical widely used mounting bracket which contains eight welding beads. DKQRL allows the reduction of structural deformation up to ∼71% and it substantially speeds up the computational time over Modified Lowest Cost Search (MLCS), Genetic Algorithm (GA), exhaustive search, and standard RL algorithm. Results of welding simulation demonstrate a reasonable agreement with real experiment in terms of structural deformation.}
}

@article{achiam2017,
      title={Constrained Policy Optimization}, 
      author={Joshua Achiam and David Held and Aviv Tamar and Pieter Abbeel},
      year={2017},
      journal= {Proceedings of the 34th International Conference on Machine Learning (ICML)}
}

@article{marek2010,
author = {Grzes, Marek},
year = {2010},
month = {03},
pages = {},
title = {Improving Exploration in Reinforcement Learning through Domain Knowledge and Parameter Analysis}
}

@misc{gym_minigrid,
  author = {Chevalier-Boisvert, Maxime and Willems, Lucas and Pal, Suman},
  title = {Minimalistic Gridworld Environment for OpenAI Gym},
  year = {2018},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/maximecb/gym-minigrid}},
}

@inproceedings{problog,
author = {De Raedt, Luc and Kimmig, Angelika and Toivonen, Hannu},
year = {2007},
month = {01},
pages = {2462-2467},
title = {ProbLog: A Probabilistic Prolog and Its Application in Link Discovery},
journal = {IJCAI}
}

@article{fierens_van,
 title={Inference and learning in probabilistic logic programs using weighted Boolean formulas}, 
 volume={15}, 
 DOI={10.1017/S1471068414000076}, 
 number={3},
 journal={Theory and Practice of Logic Programming}, 
 publisher={Cambridge University Press}, 
 author={FIERENS, DAAN and VAN DEN BROECK, GUY and RENKENS, JORIS and SHTERIONOV, DIMITAR and GUTMANN, BERND and THON, INGO and JANSSENS, GERDA and DE RAEDT, LUC},
year={2015}, 
pages={358–401}
}


@misc{dio,
  author = {Rahmouni, Samar and Reis, Giselle},
  title = {Domain Informed Oracle for Reinforcement Learning},
  year = {2022},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/natvern/Thesis}},
}
